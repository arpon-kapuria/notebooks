{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W9hWCF_UuR1l",
      "metadata": {
        "id": "W9hWCF_UuR1l"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"hf_XXXXXX\")  # Replace with your actual Hugging Face token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "283da006",
      "metadata": {
        "id": "283da006"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bac1a79a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bac1a79a",
        "outputId": "b03c321e-1a7a-4947-f7de-7dea3557775a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57335231",
      "metadata": {
        "id": "57335231"
      },
      "outputs": [],
      "source": [
        "def compute_log_prob(model, tokenizer, answer):\n",
        "    \"\"\"\n",
        "    Compute log P(answer)\n",
        "    Returns the average log probability per token.\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = tokenizer(answer, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # shift for next token prediction\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_labels = inputs['input_ids'][..., 1:].contiguous()\n",
        "\n",
        "    # Log-softmax over vocabulary\n",
        "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
        "\n",
        "    # Log probs of the actual tokens\n",
        "    token_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return token_log_probs.mean().item()\n",
        "\n",
        "\n",
        "def compute_log_prob_given_context(model, tokenizer, context, answer):\n",
        "    \"\"\"\n",
        "    Compute log P(answer | context)\n",
        "    Returns the average log probability per token given the context.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize both context and answer and move to device\n",
        "    context_inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
        "    full_text = context + \" \" + answer\n",
        "    full_inputs = tokenizer(full_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    context_len = context_inputs['input_ids'].size(1)\n",
        "    full_len = full_inputs['input_ids'].size(1)\n",
        "    answer_len = full_len - context_len\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model(**full_inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Only consider logits corresponding to answer tokens\n",
        "    answer_logits = logits[0, context_len-1:context_len-1+answer_len]\n",
        "    answer_ids = full_inputs['input_ids'][0, context_len:context_len+answer_len]\n",
        "\n",
        "    log_probs = F.log_softmax(answer_logits, dim=-1)\n",
        "    token_log_probs = log_probs.gather(dim=-1, index=answer_ids.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return token_log_probs.mean().item()\n",
        "\n",
        "\n",
        "def compute_consens_score(answer, context):\n",
        "    \"\"\"\n",
        "    Compute ConSens score: measures how much context changes the probability of an answer.\n",
        "\n",
        "    ConSens = 2 * sigmoid(log P(answer) - log P(answer|context)) - 1\n",
        "\n",
        "    Returns a value between -1 and 1\n",
        "    \"\"\"\n",
        "\n",
        "    model_name = \"EleutherAI/gpt-neo-125M\"\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map={\"\": device.type},\n",
        "        torch_dtype=torch.float16 if device.type != \"cpu\" else torch.float32,\n",
        "        use_cache=True,\n",
        "        # flash_attn=True,\n",
        "    ).to(device)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    log_pa = compute_log_prob(model, tokenizer, answer)\n",
        "    log_pac = compute_log_prob_given_context(model, tokenizer, context, answer)\n",
        "\n",
        "    r = log_pac - log_pa\n",
        "    consens_score = 2 * torch.sigmoid(torch.tensor(r).to(\"cpu\")) - 1\n",
        "\n",
        "    return consens_score.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bcdr9vWY4tJ4",
      "metadata": {
        "id": "Bcdr9vWY4tJ4"
      },
      "source": [
        "Let's say we ask:\n",
        "\n",
        "> **What is the capital of France?**\n",
        "\n",
        "1. If relevant information exists in the retrieval context and the model becomes more confident in the correct answer after seeing context -\n",
        "    * ConSens > 0 → Context is helpful → Model is using the context appropriately.\n",
        "\n",
        "2. If the model was already confident without context and the context didn’t help (or hurt) -\n",
        "    * ConSens ≈ 0 → Model possibly ignores the context → May rely on memorization.\n",
        "\n",
        "3. If the context contradicts the correct answer and the model’s confidence drops -\n",
        "    * ConSens < 0 → Context confuses or misleads the model → May signal hallucination or conflict.\n",
        "\n",
        "\n",
        "**N.B: Using more efficient model would provide more accurate score.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a58d9c30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a58d9c30",
        "outputId": "39a82200-03ef-4b41-8716-08812bb55071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context helps case - ConSens Score: 0.46\n"
          ]
        }
      ],
      "source": [
        "# Example where context should help\n",
        "\n",
        "answer1 = \"Paris is the capital of France.\"\n",
        "context1 = \"France is a country in Europe. It's capital is known for art and culture.\"\n",
        "\n",
        "score2 = compute_consens_score(answer1, context1)\n",
        "print(f\"Context helps case - ConSens Score: {score2:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ac3e949",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ac3e949",
        "outputId": "18a2848a-7654-438f-e2a8-a1d0b6eb6430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context hurts case - ConSens Score: -0.18\n"
          ]
        }
      ],
      "source": [
        "# Example where context might hurt\n",
        "\n",
        "answer2 = \"Paris is the capital of France.\"\n",
        "context2 = \"In Douglas Adams' Hitchhiker's Guide to the Galaxy, what is the answer to life, the universe, and everything?\"\n",
        "\n",
        "score3 = compute_consens_score(answer2, context2)\n",
        "print(f\"Context hurts case - ConSens Score: {score3:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
